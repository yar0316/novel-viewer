#!/usr/bin/env python3
"""
å°èª¬ãƒ‡ãƒ¼ã‚¿ã‚’Supabaseã«åŒæœŸã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub Actionsã‹ã‚‰å®Ÿè¡Œã•ã‚Œã‚‹
"""

import os
import sys
import json
import glob
import requests
import yaml
import frontmatter
from datetime import datetime
from pathlib import Path

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰Supabaseæƒ…å ±ã‚’å–å¾—
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SERVICE_KEY:
    print("âŒ Error: SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY must be set.")
    sys.exit(1)

# Supabase REST APIç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    "apikey": SERVICE_KEY,
    "Authorization": f"Bearer {SERVICE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=minimal",  # INSERTç”¨ãƒ˜ãƒƒãƒ€ãƒ¼
}

def log(message):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°å‡ºåŠ›"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def validate_novel_data(novel_data):
    """å°èª¬ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    required_fields = ['title', 'author']
    for field in required_fields:
        if field not in novel_data or not novel_data[field]:
            return False, f"Required field '{field}' is missing or empty"
    
    # tagsãŒé…åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    if 'tags' in novel_data and not isinstance(novel_data['tags'], list):
        return False, "tags must be an array"
    
    return True, "OK"

def insert_data(table_name, data):
    """Supabaseã«ãƒ‡ãƒ¼ã‚¿ã‚’INSERTã™ã‚‹"""
    if not data:
        log(f"âš ï¸  No data to insert for table '{table_name}'")
        return None
    
    # novelsãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    if table_name == 'novels':
        for i, record in enumerate(data):
            is_valid, error_msg = validate_novel_data(record)
            if not is_valid:
                log(f"âŒ Data validation failed for record {i+1}: {error_msg}")
                log(f"ğŸ” Invalid record: {json.dumps(record, indent=2, ensure_ascii=False, default=str)}")
                return None
    
    url = f"{SUPABASE_URL}/rest/v1/{table_name}"
    
    try:
        log(f"â• Inserting {len(data)} records to '{table_name}'...")
        log(f"ğŸ“¤ Request URL: {url}")
        log(f"ğŸ“‹ Headers: {json.dumps({k: v for k, v in HEADERS.items() if k != 'Authorization'}, indent=2)}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        headers_with_return = HEADERS.copy()
        headers_with_return["Prefer"] = "return=representation"  # INSERTã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        response = requests.post(url, headers=headers_with_return, json=data)
        log(f"ğŸ“¥ Response status: {response.status_code}")
        
        response.raise_for_status()
        log(f"âœ… Successfully inserted {len(data)} records to '{table_name}'")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return response.json()
        
    except requests.exceptions.RequestException as e:
        log(f"âŒ Error inserting to '{table_name}': {e}")
        if hasattr(e, 'response') and e.response:
            log(f"Response status: {e.response.status_code}")
            log(f"Response body: {e.response.text}")
            
            # Supabaseã‚¨ãƒ©ãƒ¼ã®è©³ç´°è§£æ
            try:
                error_json = e.response.json()
                log(f"ğŸ“‹ Supabase error details: {json.dumps(error_json, indent=2, ensure_ascii=False)}")
                
                # å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if 'message' in error_json:
                    log(f"ğŸ’¡ Error message: {error_json['message']}")
                if 'hint' in error_json:
                    log(f"ğŸ’¡ Hint: {error_json['hint']}")
                if 'details' in error_json:
                    log(f"ğŸ’¡ Details: {error_json['details']}")
                    
            except:
                log("ğŸ“‹ Could not parse error response as JSON")
        
        # ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°å‡ºåŠ›
        log(f"ğŸ” Data being sent to '{table_name}':")
        for i, record in enumerate(data[:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
            log(f"  Record {i+1}: {json.dumps(record, indent=2, ensure_ascii=False, default=str)}")
            # ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
            log(f"  Record {i+1} types:")
            for key, value in record.items():
                log(f"    {key}: {type(value).__name__} = {value}")
        if len(data) > 3:
            log(f"  ... and {len(data) - 3} more records")
        
        sys.exit(1)
    
    return None

def update_data(table_name, data, match_field):
    """Supabaseã®ãƒ‡ãƒ¼ã‚¿ã‚’UPDATEã™ã‚‹"""
    if not data:
        log(f"âš ï¸  No data to update for table '{table_name}'")
        return None
    
    url = f"{SUPABASE_URL}/rest/v1/{table_name}"
    
    try:
        log(f"ğŸ”„ Updating {len(data)} records in '{table_name}'...")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        headers_with_return = HEADERS.copy()
        headers_with_return["Prefer"] = "return=representation"
        
        updated_records = []
        for record in data:
            if match_field not in record:
                log(f"âš ï¸  Skipping record without {match_field}: {record}")
                continue
            
            # ãƒãƒƒãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã‚’å–å¾—ã—ã¦URLã«è¿½åŠ 
            match_value = record[match_field]
            update_url = f"{url}?{match_field}=eq.{match_value}"
            
            # ãƒãƒƒãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ï¼ˆUPDATEã§ã¯ä¸è¦ï¼‰
            update_record = {k: v for k, v in record.items() if k != match_field}
            
            response = requests.patch(update_url, headers=headers_with_return, json=update_record)
            log(f"ğŸ“¥ Update response status for {match_field}={match_value}: {response.status_code}")
            
            response.raise_for_status()
            
            if response.json():
                updated_records.extend(response.json())
        
        log(f"âœ… Successfully updated {len(updated_records)} records in '{table_name}'")
        return updated_records
        
    except requests.exceptions.RequestException as e:
        log(f"âŒ Error updating '{table_name}': {e}")
        if hasattr(e, 'response') and e.response:
            log(f"Response status: {e.response.status_code}")
            log(f"Response body: {e.response.text}")
        sys.exit(1)
    
    return None

def delete_data(table_name, ids, match_field):
    """Supabaseã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’DELETEã™ã‚‹"""
    if not ids:
        log(f"âš ï¸  No data to delete from table '{table_name}'")
        return None
    
    url = f"{SUPABASE_URL}/rest/v1/{table_name}"
    
    try:
        log(f"ğŸ—‘ï¸  Deleting {len(ids)} records from '{table_name}'...")
        
        deleted_count = 0
        for id_value in ids:
            delete_url = f"{url}?{match_field}=eq.{id_value}"
            
            response = requests.delete(delete_url, headers=HEADERS)
            log(f"ğŸ“¥ Delete response status for {match_field}={id_value}: {response.status_code}")
            
            response.raise_for_status()
            deleted_count += 1
        
        log(f"âœ… Successfully deleted {deleted_count} records from '{table_name}'")
        return deleted_count
        
    except requests.exceptions.RequestException as e:
        log(f"âŒ Error deleting from '{table_name}': {e}")
        if hasattr(e, 'response') and e.response:
            log(f"Response status: {e.response.status_code}")
            log(f"Response body: {e.response.text}")
        sys.exit(1)
    
    return None

def process_novel_directory(novel_dir):
    """å°èª¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    novel_path = Path(novel_dir)
    
    # info.ymlã‹ã‚‰å°èª¬æƒ…å ±ã‚’å–å¾—
    info_file = novel_path / "info.yml"
    if not info_file.exists():
        log(f"âš ï¸  Skipping {novel_path.name}: info.yml not found")
        return None, []
    
    try:
        with open(info_file, 'r', encoding='utf-8') as f:
            novel_data = yaml.safe_load(f)
    except Exception as e:
        log(f"âŒ Error reading {info_file}: {e}")
        return None, []
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
    required_fields = ['title', 'author', 'published']
    for field in required_fields:
        if field not in novel_data:
            log(f"âŒ Missing required field '{field}' in {info_file}")
            return None, []
    
    # publishedãŒfalseã®å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if not novel_data.get('published', False):
        log(f"âš ï¸  Skipping {novel_path.name}: published=false")
        return None, []
    
    # updatedãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
    updated = novel_data.get('updated', False)
    novel_id = novel_data.get('id')
    
    # å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã®åˆ¤å®š
    if novel_id is not None and updated:
        # IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦updated=trueã®å ´åˆã¯æ›´æ–°
        operation = 'update'
        log(f"ğŸ“ Novel '{novel_data['title']}' will be updated (id={novel_id})")
    elif novel_id is None:
        # IDãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
        operation = 'insert'
        log(f"â• Novel '{novel_data['title']}' will be inserted as new")
    else:
        # ãã‚Œä»¥å¤–ã®å ´åˆã¯ç„¡è¦–
        log(f"âš ï¸  Skipping {novel_path.name}: id exists but updated=false")
        return None, []
    
    # ä¸€æ™‚IDã‚’ä¿å­˜ï¼ˆepisodeã¨ã®é–¢é€£ä»˜ã‘ã«ä½¿ç”¨ï¼‰
    temp_novel_id = novel_id if novel_id is not None else novel_data['title']
    novel_data['temp_novel_id'] = temp_novel_id
    novel_data['operation'] = operation
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦å¤‰æ›´
    if 'description' in novel_data:
        novel_data['summary'] = novel_data.pop('description')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã«å­˜åœ¨ã—ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤
    fields_to_remove = ['status', 'published', 'updated']
    for field in fields_to_remove:
        if field in novel_data:
            del novel_data[field]
    
    # tagsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’è¨­å®š
    if 'tags' not in novel_data:
        novel_data['tags'] = []
    
    # tagsãŒæ–‡å­—åˆ—ã®å ´åˆã¯é…åˆ—ã«å¤‰æ›
    if isinstance(novel_data.get('tags'), str):
        novel_data['tags'] = [novel_data['tags']]
    
    # tagsãŒé…åˆ—ã§ãªã„å ´åˆã¯ç©ºã®é…åˆ—ã«è¨­å®š
    if not isinstance(novel_data.get('tags'), list):
        novel_data['tags'] = []
    
    # tagsã®å„è¦ç´ ãŒæ–‡å­—åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    novel_data['tags'] = [str(tag).strip() for tag in novel_data['tags'] if tag]
    
    # æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é©åˆ‡ãªTIMESTAMPå½¢å¼ã«å¤‰æ›
    if 'created_at' in novel_data:
        # YYYY-MM-DDå½¢å¼ã‚’TIMESTAMPå½¢å¼ã«å¤‰æ›
        try:
            from datetime import datetime as dt
            created_date = dt.strptime(novel_data['created_at'], '%Y-%m-%d')
            novel_data['created_at'] = created_date.isoformat() + 'Z'
        except ValueError:
            # å¤‰æ›å¤±æ•—æ™‚ã¯ç¾åœ¨æ—¥æ™‚ã‚’ä½¿ç”¨
            novel_data['created_at'] = datetime.now().isoformat() + 'Z'
    
    # ç¾åœ¨æ™‚åˆ»ã‚’æ›´æ–°æ—¥æ™‚ã¨ã—ã¦è¨­å®š
    novel_data['updated_at'] = datetime.now().isoformat() + 'Z'
    
    # ãƒ‡ãƒãƒƒã‚°: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    log(f"ğŸ” Processed novel data: {json.dumps(novel_data, indent=2, ensure_ascii=False, default=str)}")
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    episodes_data = []
    episode_files = sorted(novel_path.glob("*.md"))
    
    for episode_file in episode_files:
        try:
            with open(episode_file, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
            
            episode = post.metadata.copy()
            episode['temp_novel_id'] = temp_novel_id  # ä¸€æ™‚çš„ã«info.ymlã®IDã‚’ä¿å­˜
            episode['content'] = post.content
            
            # statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
            episode_status = episode.get('status', 'new')
            
            # statusã«å¿œã˜ãŸå‡¦ç†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            if episode_status == 'draft':
                # ä¸‹æ›¸ãã¯ç„¡è¦–
                log(f"âš ï¸  Skipping episode {episode_file.name}: status=draft")
                continue
            elif episode_status == 'deleted':
                # å‰Šé™¤å¯¾è±¡ã¨ã—ã¦ãƒãƒ¼ã‚¯
                episode['operation'] = 'delete'
            elif episode_status == 'updated':
                # æ›´æ–°å¯¾è±¡ã¨ã—ã¦ãƒãƒ¼ã‚¯
                episode['operation'] = 'update'
            elif episode_status == 'new':
                # æ–°è¦ä½œæˆå¯¾è±¡ã¨ã—ã¦ãƒãƒ¼ã‚¯
                episode['operation'] = 'insert'
            else:
                log(f"âš ï¸  Unknown status '{episode_status}' for episode {episode_file.name}, treating as 'new'")
                episode['operation'] = 'insert'
            
            # episodesãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã«å­˜åœ¨ã—ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤
            fields_to_remove = ['updated_at', 'status']
            for field in fields_to_remove:
                if field in episode:
                    del episode[field]
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            if 'id' not in episode:
                log(f"âš ï¸  Episode {episode_file.name} missing 'id', skipping")
                continue
            
            episodes_data.append(episode)
            
        except Exception as e:
            log(f"âŒ Error processing {episode_file}: {e}")
            continue
    
    log(f"ğŸ“– Processed novel '{novel_data['title']}' with {len(episodes_data)} episodes")
    return novel_data, episodes_data

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) != 2:
        print("Usage: python sync_supabase.py <data_directory>")
        sys.exit(1)
    
    data_dir = Path(sys.argv[1])
    
    if not data_dir.exists():
        log(f"âŒ Data directory {data_dir} does not exist")
        sys.exit(1)
    
    log(f"ğŸš€ Starting sync from {data_dir}")
    
    # æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™ï¼ˆnovelså»ƒæ­¢ã€ç›´æ¥æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢ï¼‰
    all_novels = []
    all_episodes = []
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã®å„æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†
    for book_dir in data_dir.iterdir():
        if book_dir.is_dir() and not book_dir.name.startswith('.'):
            # manuscript ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            manuscript_dir = book_dir / "manuscript"
            if manuscript_dir.exists() and manuscript_dir.is_dir():
                novel_data, episodes_data = process_novel_directory(manuscript_dir)
                if novel_data:
                    all_novels.append(novel_data)
                    all_episodes.extend(episodes_data)
            else:
                log(f"âš ï¸  Skipping {book_dir.name}: manuscript directory not found")
    
    if not all_novels:
        log("âš ï¸  No valid novels found to sync")
        return
    
    # Supabaseã«åŒæœŸ
    log(f"ğŸ“Š Summary: {len(all_novels)} novels, {len(all_episodes)} episodes")
    
    # å°èª¬ã®æ“ä½œåˆ¥ã«åˆ†é¡
    novels_to_insert = []
    novels_to_update = []
    temp_id_mapping = {}  # å…ƒã®IDã‚’ä¿å­˜
    
    for i, novel_data in enumerate(all_novels):
        # temp_novel_idã‚’ä¿å­˜
        temp_id = novel_data.get('temp_novel_id', novel_data.get('title'))
        temp_id_mapping[i] = temp_id
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
        novel_copy = novel_data.copy()
        operation = novel_copy.pop('operation', 'insert')
        novel_copy.pop('temp_novel_id', None)
        
        if operation == 'update':
            novels_to_update.append(novel_copy)
        else:
            novels_to_insert.append(novel_copy)
    
    # 1. å°èª¬ã®å‡¦ç†
    novel_response = None
    id_mapping = {}
    
    # æ–°è¦å°èª¬ã‚’INSERT
    if novels_to_insert:
        novel_response = insert_data("novels", novels_to_insert)
        if novel_response:
            # æ–°è¦ä½œæˆã•ã‚ŒãŸå°èª¬ã®IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            insert_index = 0
            for i, novel_data in enumerate(all_novels):
                if novel_data.get('operation') == 'insert':
                    temp_id = temp_id_mapping[i]
                    if insert_index < len(novel_response):
                        actual_id = novel_response[insert_index]['id']
                        id_mapping[temp_id] = actual_id
                        log(f"  æ–°è¦: {temp_id} â†’ {actual_id}")
                        insert_index += 1
    
    # æ—¢å­˜å°èª¬ã‚’UPDATE
    if novels_to_update:
        update_response = update_data("novels", novels_to_update, "id")
        if update_response:
            # æ›´æ–°ã•ã‚ŒãŸå°èª¬ã®IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            for i, novel_data in enumerate(all_novels):
                if novel_data.get('operation') == 'update':
                    temp_id = temp_id_mapping[i]
                    actual_id = novel_data.get('id')  # æ›´æ–°ã®å ´åˆã¯å…ƒã®IDã‚’ä½¿ç”¨
                    if actual_id:
                        id_mapping[temp_id] = actual_id
                        log(f"  æ›´æ–°: {temp_id} â†’ {actual_id}")
    
    # 2. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‡¦ç†
    if all_episodes and id_mapping:
        log("ğŸ”— Processing episodes...")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ“ä½œåˆ¥ã«åˆ†é¡
        episodes_to_insert = []
        episodes_to_update = []
        episodes_to_delete = []
        
        for episode in all_episodes:
            temp_id = episode.get('temp_novel_id')
            operation = episode.get('operation', 'insert')
            
            # novel_idã‚’å®Ÿéš›ã®IDã«æ›´æ–°
            if temp_id in id_mapping:
                episode['novel_id'] = id_mapping[temp_id]
                episode.pop('temp_novel_id', None)
                episode.pop('operation', None)
                
                if operation == 'delete':
                    episodes_to_delete.append(episode['id'])
                elif operation == 'update':
                    episodes_to_update.append(episode)
                else:  # insert or new
                    episodes_to_insert.append(episode)
            else:
                log(f"âš ï¸  Could not map episode {episode.get('id')} to novel ID")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‰Šé™¤
        if episodes_to_delete:
            delete_data("episodes", episodes_to_delete, "id")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ›´æ–°
        if episodes_to_update:
            update_data("episodes", episodes_to_update, "id")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ–°è¦ä½œæˆ
        if episodes_to_insert:
            insert_data("episodes", episodes_to_insert)
    
    log("ğŸ‰ Sync completed successfully!")

if __name__ == "__main__":
    main()