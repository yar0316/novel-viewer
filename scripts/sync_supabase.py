#!/usr/bin/env python3
"""
å°èª¬ãƒ‡ãƒ¼ã‚¿ã‚’Supabaseã«åŒæœŸã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub Actionsã‹ã‚‰å®Ÿè¡Œã•ã‚Œã‚‹
"""

import os
import sys
import json
import glob
import requests
import yaml
import frontmatter
from datetime import datetime
from pathlib import Path

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰Supabaseæƒ…å ±ã‚’å–å¾—
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SERVICE_KEY:
    print("âŒ Error: SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY must be set.")
    sys.exit(1)

# Supabase REST APIç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    "apikey": SERVICE_KEY,
    "Authorization": f"Bearer {SERVICE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "resolution=merge-duplicates",  # UPSERTã®ã‚­ãƒ¼
}

def log(message):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°å‡ºåŠ›"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def upsert_data(table_name, data):
    """Supabaseã«ãƒ‡ãƒ¼ã‚¿ã‚’UPSERTã™ã‚‹"""
    if not data:
        log(f"âš ï¸  No data to upsert for table '{table_name}'")
        return
    
    url = f"{SUPABASE_URL}/rest/v1/{table_name}"
    
    try:
        log(f"ğŸ”„ Upserting {len(data)} records to '{table_name}'...")
        log(f"ğŸ“¤ Request URL: {url}")
        log(f"ğŸ“‹ Headers: {json.dumps({k: v for k, v in HEADERS.items() if k != 'Authorization'}, indent=2)}")
        
        response = requests.post(url, headers=HEADERS, json=data)
        log(f"ğŸ“¥ Response status: {response.status_code}")
        
        response.raise_for_status()
        log(f"âœ… Successfully upserted {len(data)} records to '{table_name}'")
        
    except requests.exceptions.RequestException as e:
        log(f"âŒ Error upserting to '{table_name}': {e}")
        if hasattr(e, 'response') and e.response:
            log(f"Response status: {e.response.status_code}")
            log(f"Response body: {e.response.text}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°å‡ºåŠ›
        log(f"ğŸ” Data being sent to '{table_name}':")
        for i, record in enumerate(data[:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
            log(f"  Record {i+1}: {json.dumps(record, indent=2, ensure_ascii=False, default=str)}")
            # ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
            log(f"  Record {i+1} types:")
            for key, value in record.items():
                log(f"    {key}: {type(value).__name__} = {value}")
        if len(data) > 3:
            log(f"  ... and {len(data) - 3} more records")
        
        sys.exit(1)

def process_novel_directory(novel_dir):
    """å°èª¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    novel_path = Path(novel_dir)
    
    # info.ymlã‹ã‚‰å°èª¬æƒ…å ±ã‚’å–å¾—
    info_file = novel_path / "info.yml"
    if not info_file.exists():
        log(f"âš ï¸  Skipping {novel_path.name}: info.yml not found")
        return None, []
    
    try:
        with open(info_file, 'r', encoding='utf-8') as f:
            novel_data = yaml.safe_load(f)
    except Exception as e:
        log(f"âŒ Error reading {info_file}: {e}")
        return None, []
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
    required_fields = ['id', 'title', 'author']
    for field in required_fields:
        if field not in novel_data:
            log(f"âŒ Missing required field '{field}' in {info_file}")
            return None, []
    
    # IDã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®SERIAL PRIMARY KEYå¯¾å¿œï¼‰
    try:
        novel_data['id'] = int(novel_data['id'])
    except (ValueError, TypeError):
        log(f"âŒ Invalid ID format in {info_file}: {novel_data['id']} must be numeric")
        return None, []
    
    # ç¾åœ¨æ™‚åˆ»ã‚’æ›´æ–°æ—¥æ™‚ã¨ã—ã¦è¨­å®š
    novel_data['updated_at'] = datetime.now().isoformat()
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    episodes_data = []
    episode_files = sorted(novel_path.glob("*.md"))
    
    for episode_file in episode_files:
        try:
            with open(episode_file, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
            
            episode = post.metadata.copy()
            episode['novel_id'] = novel_data['id']
            episode['content'] = post.content
            episode['updated_at'] = datetime.now().isoformat()
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            if 'id' not in episode:
                log(f"âš ï¸  Episode {episode_file.name} missing 'id', skipping")
                continue
            
            episodes_data.append(episode)
            
        except Exception as e:
            log(f"âŒ Error processing {episode_file}: {e}")
            continue
    
    log(f"ğŸ“– Processed novel '{novel_data['title']}' with {len(episodes_data)} episodes")
    return novel_data, episodes_data

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) != 2:
        print("Usage: python sync_supabase.py <data_directory>")
        sys.exit(1)
    
    data_dir = Path(sys.argv[1])
    
    if not data_dir.exists():
        log(f"âŒ Data directory {data_dir} does not exist")
        sys.exit(1)
    
    log(f"ğŸš€ Starting sync from {data_dir}")
    
    # æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™ï¼ˆnovelså»ƒæ­¢ã€ç›´æ¥æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢ï¼‰
    all_novels = []
    all_episodes = []
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã®å„æ›¸åãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†
    for book_dir in data_dir.iterdir():
        if book_dir.is_dir() and not book_dir.name.startswith('.'):
            # manuscript ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            manuscript_dir = book_dir / "manuscript"
            if manuscript_dir.exists() and manuscript_dir.is_dir():
                novel_data, episodes_data = process_novel_directory(manuscript_dir)
                if novel_data:
                    all_novels.append(novel_data)
                    all_episodes.extend(episodes_data)
            else:
                log(f"âš ï¸  Skipping {book_dir.name}: manuscript directory not found")
    
    if not all_novels:
        log("âš ï¸  No valid novels found to sync")
        return
    
    # Supabaseã«åŒæœŸ
    log(f"ğŸ“Š Summary: {len(all_novels)} novels, {len(all_episodes)} episodes")
    
    upsert_data("novels", all_novels)
    if all_episodes:
        upsert_data("episodes", all_episodes)
    
    log("ğŸ‰ Sync completed successfully!")

if __name__ == "__main__":
    main()